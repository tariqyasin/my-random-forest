{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import feather\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim is to make a neural network to predict the MNIST data.\n",
    "First read in data (taken from Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_raw['label'].values\n",
    "X = df_raw.drop(['label'], axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough plan:\n",
    "    \n",
    "    initialise weights\n",
    "\n",
    "    training loop:\n",
    "        feedforward\n",
    "        calculate loss function\n",
    "        backpropogation\n",
    "        update weights\n",
    "\n",
    "    predict unlabelled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use a sigmoid activation function for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    g = np.zeros(z.shape)\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(rolled_weights, input_layer_size, hidden_layer_size,\n",
    "                  num_labels,X, y, lambda_=0.0):\n",
    "    \n",
    "    \"\"\"Calculates cost function and gradients of the weight matrices.\n",
    "    \n",
    "    Args:\n",
    "        rolled_weights (numpy array): 1D array containing the unrolled weight matrices.\n",
    "        num_labels (int): The number of y data labels.\n",
    "        lambda_ (float): Regularization.\n",
    "        \n",
    "    Returns:\n",
    "        J (float): Cost function. Cross-entropy.\n",
    "        grad (numpy array): 1D array containing the gradient of the unrolled weight matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unroll rolled_weights into the weight matrices.\n",
    "    Theta1 = np.reshape(rolled_weights[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    Theta2 = np.reshape(rolled_weights[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "\n",
    "    m = y.size\n",
    "    \n",
    "    # Convert y to a binary matrix with each column representing a label 0-9.\n",
    "    y_binary = np.zeros((y.size, num_labels))\n",
    "    for i, digit in enumerate(y):\n",
    "        y_binary[i, digit] = 1         \n",
    "\n",
    "    # Feed forward\n",
    "    layer1_activ = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    layer2_in = layer1_activ @ Theta1.T\n",
    "    layer2_activ = np.hstack((np.ones((X.shape[0], 1)), sigmoid(layer2_in)))\n",
    "    layer3_in = layer2_activ @ Theta2.T\n",
    "    layer3_activ = sigmoid(layer3_in)\n",
    "    delta3 = layer3_activ - y_binary\n",
    "    \n",
    "    # Cost function\n",
    "    J = - np.sum(y_binary * np.log(layer3_activ) + (1 - y_binary) * np.log(1-layer3_activ)) / np.size(y)\n",
    "    J += (lambda_ / (2*m)) * (np.sum(Theta1[:,1:]**2) + np.sum(Theta2[:,1:]**2))\n",
    "    \n",
    "    # Backpropogation. Slice out bias terms.\n",
    "    Theta2_grad  = (1/m) * delta3.T @ layer2_activ\n",
    "    Theta2_grad[:,1:] += (lambda_/m) * Theta2[:,1:]\n",
    "    \n",
    "    delta2 =  (Theta2.T @ delta3.T)[1:,:]\n",
    "    delta2 = sigmoid_gradient(layer2_in).T *  delta2\n",
    "    \n",
    "    Theta1_grad = (1/m) * delta2 @ layer1_activ\n",
    "    Theta1_grad[:,1:] += (lambda_/m) * Theta1[:,1:]\n",
    "\n",
    "    # Unroll gradients\n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()]) \n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(nodes_in, nodes_out, epsilon=0.12):\n",
    "    \"\"\"Randomly nitializes the weight matrix with L_in input nodes and L_out output nodes.\"\"\"\n",
    "    W = np.zeros((nodes_out, 1 + nodes_in))\n",
    "    W = np.random.rand(nodes_out, 1 + nodes_in) * 2 * epsilon - epsilon\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "options= {'maxiter': 300}\n",
    "input_layer_size = X.shape[1]\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "lambda_ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_Theta1 = initialize_weights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = initialize_weights(hidden_layer_size, num_labels)\n",
    "initial_rolled_weights = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize with scipy.optimize\n",
    "costFunction = lambda p: cost_function(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "res = optimize.minimize(costFunction, initial_rolled_weights, jac=True, \n",
    "                        method='TNC', options=options)\n",
    "\n",
    "# Get result of optimization.\n",
    "rolled_weights = res.x\n",
    "Theta1 = np.reshape(rolled_weights[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "Theta2 = np.reshape(rolled_weights[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "    \"\"\"Feedforward to predict digits.\"\"\"\n",
    "    m = X.shape[0]\n",
    "    num_labels = Theta2.shape[0]\n",
    "\n",
    "    p = np.zeros(m)\n",
    "    h1 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), X], axis=1), Theta1.T))\n",
    "    h2 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), h1], axis=1), Theta2.T))\n",
    "    p = np.argmax(h2, axis=1)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 86.819048\n"
     ]
    }
   ],
   "source": [
    "pred = predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred == y) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for a first attempt.\n",
    "\n",
    "To do:\n",
    "- Make a cross-validation set (or bootstrap).\n",
    "- Adjust with hidden layer size, regularization and training time.\n",
    "- Should be able to get up to ~95% on test data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
